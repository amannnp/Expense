{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3e4e2-4935-4d0d-8a3b-a093ea53e559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581a291-8927-4c00-b717-1745e086b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5705212-113d-4915-92bc-49c7644d7424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b3530-adc4-473e-9505-15b5a05d26d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb753133-4822-4044-97a2-e0c1b1a6baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7154 - loss: 1.5533 - val_accuracy: 1.0000 - val_loss: 0.0183\n",
      "Epoch 2/5\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 1.0000 - val_loss: 0.0037\n",
      "Epoch 3/5\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 4/5\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 8.8026e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.2031e-04 - val_accuracy: 1.0000 - val_loss: 5.5829e-04\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\AppData\\Local\\Temp\\tmps3r_rfsa\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\AppData\\Local\\Temp\\tmps3r_rfsa\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\KIIT\\AppData\\Local\\Temp\\tmps3r_rfsa'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 96), dtype=tf.float32, name='text_input'), TensorSpec(shape=(None, 1), dtype=tf.float32, name='amount_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2330600942736: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  2330600941968: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  2330600941200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2330600942544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2330600943888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2330600941776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2330600942352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2330600944656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ All files saved:\n",
      "- expense_model.tflite\n",
      "- amount_normalizer.npy\n",
      "- tfidf_vocab.json\n",
      "- label_classes.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Normalization, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "CSV_PATH = 'expense_dataset.csv'\n",
    "TFLITE_MODEL_PATH = 'expense_model.tflite'\n",
    "TFIDF_VOCAB_PATH = 'tfidf_vocab.json'\n",
    "SCALER_PATH = 'amount_normalizer.npy'\n",
    "LABEL_ENCODER_PATH = 'label_classes.json'\n",
    "# ============================\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df['text'] = df['merchant'].astype(str) + ' ' + df['description'].astype(str)\n",
    "texts = df['text'].values\n",
    "amounts = df['amount'].values\n",
    "labels = df['category'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# TF-IDF for text\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_text = tfidf.fit_transform(texts).toarray()\n",
    "\n",
    "# Save vocab\n",
    "with open(TFIDF_VOCAB_PATH, 'w') as f:\n",
    "    json.dump({k: int(v) for k, v in tfidf.vocabulary_.items()}, f)\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, X_amount_train, X_amount_test, y_train, y_test = train_test_split(\n",
    "    X_text, amounts, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize amount\n",
    "amount_normalizer = Normalization()\n",
    "amount_normalizer.adapt(X_amount_train.reshape(-1, 1))\n",
    "\n",
    "# Save scaler stats\n",
    "np.save(SCALER_PATH, {\n",
    "    'mean': amount_normalizer.mean.numpy().tolist(),\n",
    "    'var': amount_normalizer.variance.numpy().tolist()\n",
    "})\n",
    "\n",
    "# Save label classes\n",
    "with open(LABEL_ENCODER_PATH, 'w') as f:\n",
    "    json.dump(label_encoder.classes_.tolist(), f)\n",
    "\n",
    "# Build model\n",
    "text_input = Input(shape=(X_text_train.shape[1],), name='text_input')\n",
    "amount_input = Input(shape=(1,), dtype=tf.float32, name='amount_input')\n",
    "x_amount = amount_normalizer(amount_input)\n",
    "x = Concatenate()([text_input, x_amount])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[text_input, amount_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit({'text_input': X_text_train, 'amount_input': X_amount_train}, y_train,\n",
    "          epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ All files saved:\")\n",
    "print(\"- expense_model.tflite\")\n",
    "print(\"- amount_normalizer.npy\")\n",
    "print(\"- tfidf_vocab.json\")\n",
    "print(\"- label_classes.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe93c8-41b7-494d-aea0-5ecbff1595f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
